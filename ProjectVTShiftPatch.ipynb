{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import math\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nimport tensorflow_addons as tfa\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras import layers\nimport os\nimport cv2\nimport platform\nimport numpy as np\nimport pandas as pd\nimport random\nimport matplotlib.pyplot as plt\nfrom IPython.display import clear_output\n\n# # Setting seed for reproducibiltiy\n# SEED = 42\n# keras.utils.set_random_seed(SEED)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-26T00:50:14.082304Z","iopub.execute_input":"2022-04-26T00:50:14.082854Z","iopub.status.idle":"2022-04-26T00:50:18.616108Z","shell.execute_reply.started":"2022-04-26T00:50:14.082810Z","shell.execute_reply":"2022-04-26T00:50:18.615360Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import math\nimport os\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\n\nclass_label = {3: ('train_COVIDx9A.txt', 'test_COVIDx9A.txt'),\n               2: ('train.txt', 'test.txt')\n               }\n\nplatform_path = {'Local': './data/',\n                 'Kaggle': '/kaggle/input/covidx-cxr2/',\n                 'Colab': './content/'\n                 }\n\nlabel_colnames = ['patient_id', 'filename', 'class', 'data_source']\n\nlabel_encode = {'positive': 1,\n                'negative': 0,\n                'COVID-19': 2,\n                'normal': 0\n                }\n\n\nclass DataLoader():\n    \"\"\"\n    The DataLoader class handles loading image data from respective folders regardless of platform while performing act-\n    ions such as bootstrapping and sampling.  \n\n    Attributes:\n        platform (str): Name of the platform, options are [\"Local\", \"Kaggle\", \"Colab\"]\n        n_classes (int): Number of classfication classes, options are [2, 3]\n        data_dir (str): Folder directory containing the images, must contain the ./train/ and ./test/ folder\n        txt_dir (str): Folder directory containing the label files such as \"train_COVIDx9A.txt\", by default = data_dir\n        img_size (int): The final output image size after cropping and resizing\n        combined (bool): Determines whether to combine original data's train and test sets for custom test split\n        channels (str): Determines whether to output RGB or Greyscale (L) images\n    \"\"\"\n    def __init__(self, platform: str = 'Local', n_classes: int = 2, data_dir: str = None, txt_dir: str = None,\n                 img_size: int = 224, combined: bool = True, channels: str = 'RGB'):\n\n        # data validation checks\n        assert platform in platform_path, 'Platforms must be in [\"Local\", \"Kaggle\", \"Colab\"]'\n        self.platform = platform\n\n        assert n_classes in [2, 3], 'n_classes must be in [2, 3]'\n        self.n_classes = n_classes\n\n        # if directories not specified, take default\n        if data_dir is None:\n            self.data_dir = platform_path.get(platform)\n        else:\n            self.data_dir = data_dir\n\n        if txt_dir is None:\n            self.txt_dir = platform_path.get(platform)\n        else:\n            self.txt_dir = txt_dir\n\n        self.img_size = img_size\n        self.combined = combined\n\n        assert channels in ['L', 'RGB'], \"channels must be in ['L', 'RGB']\"\n        self.channels = channels\n\n        # check if either label file is in txt_dir folder    \n        for i in range(2):\n            if not os.path.exists(os.path.join(self.txt_dir, class_label.get(self.n_classes)[i])):\n                raise FileNotFoundError(class_label.get(self.n_classes)[i] + ' not found in ' + self.txt_dir)\n\n        # print a summary message\n        print(f'Platform: {self.platform}\\nNum Classes: {self.n_classes}')\n        print(f'Data Folder: {self.data_dir}\\nLabel Folder: {self.txt_dir}')\n        print(f'Image size: {self.img_size}\\nCombined: {self.combined}\\nImage Channels: {self.channels}')\n\n\n    def __crop_image(self, img: Image) -> Image:\n        \"\"\"\n        Crop and resize image to a square\n        The image is cropped at the centre using the shorter length.\n        It's then resized to the proposed dimensions.\n\n        Args:\n            img (Image): a PIL.Image object to be resized\n\n        Returns:\n            image_final (Image): a PIL.Image object that's resized\n        \"\"\"\n        width, height = img.size\n        diff = abs(width-height)\n\n        # initialise final image parameters\n        left, right, top, bottom = 0, width, 0, height\n\n        # crop based on whether the difference in dimensions is odd or even\n        if diff % 2 == 0:\n            if width >= height:\n                bottom = height\n                left = diff / 2\n                right = width - left\n            elif height > width:\n                top = diff / 2\n                bottom = height - top\n                right = width\n        else:\n            if width > height:\n                bottom = height\n                left = diff / 2 + 0.5\n                right = width - left + 1\n            elif height > width:\n                top = diff / 2 + 0.5\n                bottom = height - top + 1\n                right = width\n\n        # crop image into a square\n        img_cropped = img.crop((left, top, right, bottom))\n        # resize to desired shape\n        img_final = img_cropped.resize((self.img_size, self.img_size))\n        \n        return img_final\n\n\n    def __load_metadata(self) -> pd.DataFrame:\n        \"\"\"\n        Loads the label txt files, return them as separate or combined pd.DataFrame\n\n        Returns:\n            train_df (pd.DataFrame): DF of all training images file names and labels\n            test_df (pd.DataFrame): DF of all test images file names and labels\n            combined_df (pd.DataFrame): DF of combined images file names and labels\n        \"\"\"\n        # name of the label files\n        train_filename, test_filename = class_label.get(self.n_classes)\n        # read in label files\n        train_df = pd.read_csv(os.path.join(self.txt_dir, train_filename), header=None, sep=' ')\n        test_df = pd.read_csv(os.path.join(self.txt_dir, test_filename), header=None, sep=' ')\n\n        train_df.columns = label_colnames\n        test_df.columns = label_colnames\n\n        # assign file path for each image\n        train_df['filepath'] = train_df.apply(lambda row: os.path.join(self.data_dir, 'train/', row['filename']), axis=1)\n        test_df['filepath'] = test_df.apply(lambda row: os.path.join(self.data_dir, 'test/', row['filename']), axis=1)\n\n        # combine all images metadata\n        combined_df = pd.concat([train_df, test_df], ignore_index=True)\n\n        # return combined or separate based on argument\n        if self.combined:\n            return combined_df\n        else:\n            return train_df, test_df\n\n\n    def __bootstrap_sample(self, label_df: pd.DataFrame, rand_state: int,\n                           sample_size: int, sample_percent: float) -> pd.DataFrame:\n        \"\"\"\n        Creates a bootstrapped training sample from a given population.\n        Patients with multiple images have one image randomly selected with the remainder dropped from the data.\n        The data is samples with replacement with an equal number of positive/negative images.\n        The validation set is from remaining COVID-19 images and a sample of non-COVID.\n        \n        Arguments:\n            label_df (pd.DataFrame): DF of image metadata\n            rand_state (int): Random state for Pandas sampling\n            sample_size (int): Total number of images in the training set\n            sample_percent (float): Percentage of training images vs total images, if specified, overrides sample_size\n            \n        Returns:\n            df_train (pd.DataFrame): DF of images metadata for training after bootstrapping\n            df_val (pd.DataFrame): DF of images metadata for validation after bootstrapping\n        \"\"\"\n        # defaults to sample_size, unless sample_percent is specified\n        if sample_percent is not None:\n            sample_size = math.floor(label_df.shape[0] * sample_percent)\n        else:\n            assert sample_size <= label_df.shape[0], f'sample_size must be no larger than {label_df.shape[0]}'\n\n        sample_half = math.floor(sample_size/2)\n\n        # randomly pick one image from a patient and drop the rest from the sample population\n        patient_img_count = label_df[['patient_id', 'filename']].groupby(['patient_id']).count().reset_index()\n        duplicate_patients = patient_img_count['patient_id'][patient_img_count['filename'] > 1]\n        \n        for patient in duplicate_patients.unique():\n            sample_row = label_df.loc[label_df['patient_id'] == patient, :].sample(n=1, random_state=rand_state)\n            label_df = label_df.loc[label_df['patient_id'] != patient, :]\n            label_df = pd.concat([label_df, sample_row], axis=0)\n\n        # sample with replacement for positive/negative classes to create training data\n        df_train_positive = label_df[label_df['class'] == 'positive'].sample(n=sample_half,\n                                                                             replace=True, random_state=rand_state)\n        df_train_negative = label_df[label_df['class'] == 'negative'].sample(n=sample_half,\n                                                                             replace=True, random_state=rand_state)\n        \n        df_train = pd.concat([df_train_positive, df_train_negative], axis=0)\n        df_remaining = label_df.loc[label_df['filename'].isin(df_train['filename']) == False, :]\n        \n        # remaining has lots of negative samples so we sample without replacement to match positive samples\n        df_val_size = min(df_train_positive.shape[0], df_remaining[df_remaining['class']=='positive'].shape[0])\n\n        df_val_positive = df_remaining.loc[df_remaining['class'] == 'positive', :].sample(n=df_val_size,\n                                                                                          replace=False,\n                                                                                          random_state=rand_state)\n        df_val_negative = df_remaining.loc[df_remaining['class'] == 'negative', :].sample(n=df_val_positive.shape[0],\n                                                                                          replace=False,\n                                                                                          random_state=rand_state)\n        \n        df_val = pd.concat([df_val_positive, df_val_negative], axis=0)\n        \n        # shuffle datasets before returning\n        df_train = df_train.sample(frac=1, random_state=rand_state).reset_index(drop=True)\n        df_val = df_val.sample(frac=1, random_state=rand_state).reset_index(drop=True)\n\n        return df_train, df_val\n\n\n    def __test_split(self, test_percent: float) -> pd.DataFrame:\n        \"\"\"\n        Split out the holdout test set from the whole data\n        This is only for when the original data is combined, otherwise the original data defined the holdout test set\n        \n        Arguments:\n            test_percent (float): Percentage of holdout images vs total images\n\n        Returns:\n            df_train_val (pd.DataFrame): DF of images metadata for training and validation\n            df_test (pd.DataFrame): DF of images metadata for holdout testing\n        \"\"\"\n        assert test_percent > 0 and test_percent <= 1.00, 'test_percent must be in range (0, 1.00]'\n\n        # load in combined data\n        label_df = self.__load_metadata()\n        test_size = math.floor(label_df.shape[0] * test_percent)\n\n        # obtain the proportion of positive class in the data\n        positive_percent = label_df['class'].value_counts(normalize=True)['positive']\n        # determine the number of positive images in the holdout set for balanced test data\n        positive_size = math.floor(test_size * positive_percent)\n\n        positive_patients = label_df.loc[label_df['class'] == 'positive', :]\n\n        # DF of positive patients and their positive image count, then random shuffle to be sampled\n        positive_img_count = positive_patients[['patient_id', 'filename']].groupby(['patient_id']).count().reset_index()\n        positive_img_count = positive_img_count.sample(frac=1, random_state=50)\n\n\n        # sample positive patients until total number of images equals required number\n        positive_count = 0\n        positive_patients = []\n\n        for row in positive_img_count.iterrows():\n            positive_count += row[1]['filename']\n            positive_patients.append(row[1]['patient_id'])\n            if positive_count > positive_size:\n                break\n        \n        # repeat the process above for patients with negative images, however, remove any patients already picked\n        # this is to avoid cases where someone has images in both class, so they don't get picked twice\n        negative_patients = label_df.loc[(label_df['class'] == 'negative') &\n                                         (~label_df['patient_id'].isin(positive_patients)), :]\n\n        negative_img_count = negative_patients[['patient_id', 'filename']].groupby(['patient_id']).count().reset_index()\n        negative_img_count = negative_img_count.sample(frac=1, random_state=50)\n\n        negative_count = 0\n        negative_patients = []\n\n        for row in negative_img_count.iterrows():\n            negative_count += row[1]['filename']\n            negative_patients.append(row[1]['patient_id'])\n            if negative_count + positive_count > test_size:\n                break\n\n        # combined all patients picked and filter out their iamges\n        test_patients = positive_patients + negative_patients\n\n        df_test = label_df[label_df['patient_id'].isin(test_patients)]\n        df_train_val = pd.concat([label_df, df_test, df_test]).drop_duplicates(keep=False)\n\n        return df_train_val, df_test\n    \n    \n    def __train_test_split(self, rand_state: int, bootstrap: bool,\n                           sample_size: int, sample_percent: float, test_percent: float) -> pd.DataFrame:\n        \"\"\"\n        Split to train, val, test based on user choices\n        \n        Arguments:\n            rand_state (int): Random state that will determine the images selected\n            bootstrap (bool): Determines whether to bootstrap the data or not\n            sample_size (int): Total number of images in the training set\n            sample_percent (float): Percentage of training images vs total images, if specified, overrides sample_size\n            test_percent (float): Percentage of holdout images vs total images\n\n        Returns:\n            df_train (pd.DataFrame): DF of images metadata for training \n            df_val (pd.DataFrame): DF of images metadata for validation\n            df_test (pd.DataFrame): DF of images metadata for holdout testing\n        \"\"\"\n        if self.combined:\n            df_train_val, df_test = self.__test_split(test_percent=test_percent)\n        else:\n            df_train_val, df_test = self.__load_metadata(combined=self.combined)\n\n        df_train, df_val = self.__bootstrap_sample(label_df=df_train_val, rand_state=rand_state,\n                                                   sample_size=sample_size, sample_percent=sample_percent)\n\n        return df_train, df_val, df_test\n\n\n    def __load_images(self, image_df: pd.DataFrame) -> np.array:\n        \"\"\"\n        Loads in the images based on the input image metadata\n\n        Arguments:\n            image_df (pd.DataFrame): DF of images metadata, which includes their file paths\n\n        Returns:\n            images (np.array): A (no of images) x (img_size^2 x channels) array containing images specified in image_df\n        \"\"\"\n        images = []\n\n        for idx, file_path in enumerate(image_df['filepath']):\n            img = Image.open(file_path)\n            img = img.convert(self.channels)\n            img_final = self.__crop_image(img)\n            img_array = np.asarray(img_final).flatten()\n\n            images.append(img_array)\n\n        images = np.array(images)\n\n        return images\n\n\n    def __load_labels(self, image_df: pd.DataFrame) -> np.array:\n        \"\"\"\n        Loads in the image labels based on the input image metadata\n\n        Arguments:\n            image_df (pd.DataFrame): DF of images metadata, which includes their file paths\n\n        Returns:\n            labels (np.array): A (no of images x 1) array containing image labels\n        \"\"\"\n        labels = []\n\n        for label in image_df['class']:\n            labels.append(label_encode.get(label))\n\n        labels = np.array(labels)\n\n        return labels\n\n\n    def load_train_val(self, rand_state: int, bootstrap: bool = True, \n                       sample_size: int = 20000, sample_percent: float = None) -> np.array:\n        \"\"\"\n        Loads in the training and validation images based on the input image metadata\n\n        Arguments:\n            rand_state (int): Random state that will determine the images selected\n            bootstrap (bool): Determines whether to bootstrap the data or not\n            sample_size (int): Total number of images in the training set\n            sample_percent (float): Percentage of training images vs total images, if specified, overrides sample_size\n\n        Returns:\n            X_train (np.array): A (no of images) x (img_size^2 x channels) array containing training images\n            X_val (np.array): A (no of images) x (img_size^2 x channels) array containing validation images\n            Y_train (np.array): A (no of images x 1) array containing training image labels\n            Y_val (np.array): A (no of images x 1) array containing validation image labels\n        \"\"\"\n        if sample_percent is not None:\n            assert sample_percent > 0 and sample_percent <= 1.00, 'sample_percent must be in range (0, 1.00]'\n\n        df_train, df_val, _ = self.__train_test_split(rand_state=rand_state, bootstrap=bootstrap,\n                                                            sample_size=sample_size, sample_percent=sample_percent,\n                                                            test_percent=0.1)\n\n        X_train = self.__load_images(df_train)\n        X_val = self.__load_images(df_val)\n        \n        Y_train = self.__load_labels(df_train)\n        Y_val = self.__load_labels(df_val)\n\n        return X_train, X_val, Y_train, Y_val\n\n\n    def load_test(self, test_percent: float = 0.1) -> np.array:\n        \"\"\"\n        Loads in the holdout test images.\n        The images loaded are always fixed given the same test_percent\n\n        Arguments:\n            test_percent (float): Percentage of holdout images vs total images\n\n        Returns:\n            X_test (np.array): A (no of images) x (img_size^2 x channels) array containing holdout test images\n            Y_test (np.array): A (no of images x 1) array containing holdout test image labels   \n        \"\"\"\n        assert test_percent > 0 and test_percent <= 1.00, 'test_percent must be in range (0, 1.00]'\n\n        _, _, df_test = self.__train_test_split(rand_state=1, bootstrap=True,\n                                                sample_size=3000, sample_percent=None, test_percent=test_percent)\n\n        X_test = self.__load_images(df_test)\n        Y_test = self.__load_labels(df_test)\n\n        return X_test, Y_test","metadata":{"execution":{"iopub.status.busy":"2022-04-26T00:50:25.113924Z","iopub.execute_input":"2022-04-26T00:50:25.114288Z","iopub.status.idle":"2022-04-26T00:50:25.201436Z","shell.execute_reply.started":"2022-04-26T00:50:25.114247Z","shell.execute_reply":"2022-04-26T00:50:25.200518Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"image_loader = DataLoader(platform='Kaggle', # [\"Local\", \"Kaggle\", \"Colab\"]\n                          n_classes=2,      # [2, 3] only 2 works for now\n                          data_dir='/kaggle/input/covidx-cxr2/',    # See next cell\n                          txt_dir='/kaggle/input/covidx-cxr2/',     # See next cell\n                          img_size=224,     \n                          combined=True,    \n                          channels='RGB') ","metadata":{"execution":{"iopub.status.busy":"2022-04-26T00:50:35.676267Z","iopub.execute_input":"2022-04-26T00:50:35.676563Z","iopub.status.idle":"2022-04-26T00:50:35.688014Z","shell.execute_reply.started":"2022-04-26T00:50:35.676524Z","shell.execute_reply":"2022-04-26T00:50:35.686657Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import time\nt = time.time()\nX_train, X_val, Y_train, Y_val = image_loader.load_train_val(rand_state=42, sample_size=25000)\nprint(X_train.shape)\nprint(X_val.shape)\nprint(time.time() - t)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T00:50:38.485729Z","iopub.execute_input":"2022-04-26T00:50:38.486007Z","iopub.status.idle":"2022-04-26T01:05:38.026670Z","shell.execute_reply.started":"2022-04-26T00:50:38.485975Z","shell.execute_reply":"2022-04-26T01:05:38.025753Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"X_test, Y_test = image_loader.load_test()\nprint(X_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:05:48.670643Z","iopub.execute_input":"2022-04-26T01:05:48.671449Z","iopub.status.idle":"2022-04-26T01:07:43.747963Z","shell.execute_reply.started":"2022-04-26T01:05:48.671409Z","shell.execute_reply":"2022-04-26T01:07:43.747140Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"X_train = np.asarray(X_train).reshape(-1,224,224,3)\nY_train = np.asarray(Y_train)\nprint(X_train.shape) #  data*224*224*3(height*width*channel)\nprint(Y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:09:14.325127Z","iopub.execute_input":"2022-04-26T01:09:14.325790Z","iopub.status.idle":"2022-04-26T01:09:14.332734Z","shell.execute_reply.started":"2022-04-26T01:09:14.325746Z","shell.execute_reply":"2022-04-26T01:09:14.331980Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"X_val = np.asarray(X_val).reshape(-1,224,224,3)\nY_val = np.asarray(Y_val)\nprint(X_val.shape)\nprint(Y_val.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:09:17.697807Z","iopub.execute_input":"2022-04-26T01:09:17.698399Z","iopub.status.idle":"2022-04-26T01:09:17.703960Z","shell.execute_reply.started":"2022-04-26T01:09:17.698355Z","shell.execute_reply":"2022-04-26T01:09:17.703078Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"X_test = np.asarray(X_test).reshape(-1,224,224,3)\nY_test = np.asarray(Y_test)\nprint(X_test.shape)\nprint(Y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:09:20.081022Z","iopub.execute_input":"2022-04-26T01:09:20.081788Z","iopub.status.idle":"2022-04-26T01:09:20.087866Z","shell.execute_reply.started":"2022-04-26T01:09:20.081748Z","shell.execute_reply":"2022-04-26T01:09:20.087027Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"NUM_CLASSES = 100\nINPUT_SHAPE = (224, 224, 3)\n# DATA\nBUFFER_SIZE = 512\nBATCH_SIZE = 256\n\n# AUGMENTATION\nIMAGE_SIZE = 72\nPATCH_SIZE = 6\nNUM_PATCHES = (IMAGE_SIZE // PATCH_SIZE) ** 2\n\n# OPTIMIZER\nLEARNING_RATE = 0.001\nWEIGHT_DECAY = 0.0001\n\n# TRAINING\nEPOCHS = 50\n\n# ARCHITECTURE\nLAYER_NORM_EPS = 1e-6\nTRANSFORMER_LAYERS = 8\nPROJECTION_DIM = 64\nNUM_HEADS = 4\nTRANSFORMER_UNITS = [\n    PROJECTION_DIM * 2,\n    PROJECTION_DIM,\n]\nMLP_HEAD_UNITS = [2048, 1024]","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:09:22.862180Z","iopub.execute_input":"2022-04-26T01:09:22.862467Z","iopub.status.idle":"2022-04-26T01:09:22.868378Z","shell.execute_reply.started":"2022-04-26T01:09:22.862435Z","shell.execute_reply":"2022-04-26T01:09:22.867646Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"data_augmentation = keras.Sequential(\n    [\n        layers.Normalization(),\n        layers.Resizing(IMAGE_SIZE, IMAGE_SIZE),\n        layers.RandomFlip(\"horizontal\"),\n        layers.RandomRotation(factor=0.02),\n        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n    ],\n    name=\"data_augmentation\",\n)\n# Compute the mean and the variance of the training data for normalization.\ndata_augmentation.layers[0].adapt(X_train)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:09:26.230282Z","iopub.execute_input":"2022-04-26T01:09:26.230950Z","iopub.status.idle":"2022-04-26T01:09:38.164239Z","shell.execute_reply.started":"2022-04-26T01:09:26.230908Z","shell.execute_reply":"2022-04-26T01:09:38.163402Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class ShiftedPatchTokenization(layers.Layer):\n    def __init__(\n        self,\n        image_size=IMAGE_SIZE,\n        patch_size=PATCH_SIZE,\n        num_patches=NUM_PATCHES,\n        projection_dim=PROJECTION_DIM,\n        vanilla=False,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.vanilla = vanilla  # Flag to swtich to vanilla patch extractor\n        self.image_size = image_size\n        self.patch_size = patch_size\n        self.half_patch = patch_size // 2\n        self.flatten_patches = layers.Reshape((num_patches, -1))\n        self.projection = layers.Dense(units=projection_dim)\n        self.layer_norm = layers.LayerNormalization(epsilon=LAYER_NORM_EPS)\n\n    def crop_shift_pad(self, images, mode):\n        # Build the diagonally shifted images\n        if mode == \"left-up\":\n            crop_height = self.half_patch\n            crop_width = self.half_patch\n            shift_height = 0\n            shift_width = 0\n        elif mode == \"left-down\":\n            crop_height = 0\n            crop_width = self.half_patch\n            shift_height = self.half_patch\n            shift_width = 0\n        elif mode == \"right-up\":\n            crop_height = self.half_patch\n            crop_width = 0\n            shift_height = 0\n            shift_width = self.half_patch\n        else:\n            crop_height = 0\n            crop_width = 0\n            shift_height = self.half_patch\n            shift_width = self.half_patch\n\n        # Crop the shifted images and pad them\n        crop = tf.image.crop_to_bounding_box(\n            images,\n            offset_height=crop_height,\n            offset_width=crop_width,\n            target_height=self.image_size - self.half_patch,\n            target_width=self.image_size - self.half_patch,\n        )\n        shift_pad = tf.image.pad_to_bounding_box(\n            crop,\n            offset_height=shift_height,\n            offset_width=shift_width,\n            target_height=self.image_size,\n            target_width=self.image_size,\n        )\n        return shift_pad\n\n    def call(self, images):\n        if not self.vanilla:\n            # Concat the shifted images with the original image\n            images = tf.concat(\n                [\n                    images,\n                    self.crop_shift_pad(images, mode=\"left-up\"),\n                    self.crop_shift_pad(images, mode=\"left-down\"),\n                    self.crop_shift_pad(images, mode=\"right-up\"),\n                    self.crop_shift_pad(images, mode=\"right-down\"),\n                ],\n                axis=-1,\n            )\n        # Patchify the images and flatten it\n        patches = tf.image.extract_patches(\n            images=images,\n            sizes=[1, self.patch_size, self.patch_size, 1],\n            strides=[1, self.patch_size, self.patch_size, 1],\n            rates=[1, 1, 1, 1],\n            padding=\"VALID\",\n        )\n        flat_patches = self.flatten_patches(patches)\n        if not self.vanilla:\n            # Layer normalize the flat patches and linearly project it\n            tokens = self.layer_norm(flat_patches)\n            tokens = self.projection(tokens)\n        else:\n            # Linearly project the flat patches\n            tokens = self.projection(flat_patches)\n        return (tokens, patches)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:09:40.473570Z","iopub.execute_input":"2022-04-26T01:09:40.473850Z","iopub.status.idle":"2022-04-26T01:09:40.489117Z","shell.execute_reply.started":"2022-04-26T01:09:40.473819Z","shell.execute_reply":"2022-04-26T01:09:40.488096Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Get a random image from the training dataset\n# and resize the image\nimage = X_train[np.random.choice(range(X_train.shape[0]))]\nresized_image = tf.image.resize(\n    tf.convert_to_tensor([image]), size=(IMAGE_SIZE, IMAGE_SIZE)\n)\n\n# Shifted Patch Tokenization: This layer takes the image, shifts it\n# diagonally and then extracts patches from the concatinated images\n(token, patch) = ShiftedPatchTokenization(vanilla=False)(resized_image / 255.0)\n(token, patch) = (token[0], patch[0])\nn = patch.shape[0]\nshifted_images = [\"ORIGINAL\", \"LEFT-UP\", \"LEFT-DOWN\", \"RIGHT-UP\", \"RIGHT-DOWN\"]\nfor index, name in enumerate(shifted_images):\n    print(name)\n    count = 1\n    plt.figure(figsize=(4, 4))\n    for row in range(n):\n        for col in range(n):\n            plt.subplot(n, n, count)\n            count = count + 1\n            image = tf.reshape(patch[row][col], (PATCH_SIZE, PATCH_SIZE, 5 * 3))\n            plt.imshow(image[..., 3 * index : 3 * index + 3])\n            plt.axis(\"off\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:09:45.117650Z","iopub.execute_input":"2022-04-26T01:09:45.118524Z","iopub.status.idle":"2022-04-26T01:10:09.212758Z","shell.execute_reply.started":"2022-04-26T01:09:45.118468Z","shell.execute_reply":"2022-04-26T01:10:09.212073Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class PatchEncoder(layers.Layer):\n    def __init__(\n        self, num_patches=NUM_PATCHES, projection_dim=PROJECTION_DIM, **kwargs\n    ):\n        super().__init__(**kwargs)\n        self.num_patches = num_patches\n        self.position_embedding = layers.Embedding(\n            input_dim=num_patches, output_dim=projection_dim\n        )\n        self.positions = tf.range(start=0, limit=self.num_patches, delta=1)\n\n    def call(self, encoded_patches):\n        encoded_positions = self.position_embedding(self.positions)\n        encoded_patches = encoded_patches + encoded_positions\n        return encoded_patches","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:10:13.716832Z","iopub.execute_input":"2022-04-26T01:10:13.717462Z","iopub.status.idle":"2022-04-26T01:10:13.724089Z","shell.execute_reply.started":"2022-04-26T01:10:13.717420Z","shell.execute_reply":"2022-04-26T01:10:13.722907Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"class MultiHeadAttentionLSA(tf.keras.layers.MultiHeadAttention):\n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        # The trainable temperature term. The initial value is\n        # the square root of the key dimension.\n        self.tau = tf.Variable(math.sqrt(float(self._key_dim)), trainable=True)\n\n    def _compute_attention(self, query, key, value, attention_mask=None, training=None):\n        query = tf.multiply(query, 1.0 / self.tau)\n        attention_scores = tf.einsum(self._dot_product_equation, key, query)\n        attention_scores = self._masked_softmax(attention_scores, attention_mask)\n        attention_scores_dropout = self._dropout_layer(\n            attention_scores, training=training\n        )\n        attention_output = tf.einsum(\n            self._combine_equation, attention_scores_dropout, value\n        )\n        return attention_output, attention_scores","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:10:16.647900Z","iopub.execute_input":"2022-04-26T01:10:16.648690Z","iopub.status.idle":"2022-04-26T01:10:16.656766Z","shell.execute_reply.started":"2022-04-26T01:10:16.648637Z","shell.execute_reply":"2022-04-26T01:10:16.655943Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def mlp(x, hidden_units, dropout_rate):\n    for units in hidden_units:\n        x = layers.Dense(units, activation=tf.nn.gelu)(x)\n        x = layers.Dropout(dropout_rate)(x)\n    return x\n\n\n# Build the diagonal attention mask\ndiag_attn_mask = 1 - tf.eye(NUM_PATCHES)\ndiag_attn_mask = tf.cast([diag_attn_mask], dtype=tf.int8)","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:10:19.409299Z","iopub.execute_input":"2022-04-26T01:10:19.410211Z","iopub.status.idle":"2022-04-26T01:10:19.429251Z","shell.execute_reply.started":"2022-04-26T01:10:19.410163Z","shell.execute_reply":"2022-04-26T01:10:19.427858Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def create_vit_classifier(vanilla=False):\n    inputs = layers.Input(shape=INPUT_SHAPE)\n    # Augment data.\n    augmented = data_augmentation(inputs)\n    # Create patches.\n    (tokens, _) = ShiftedPatchTokenization(vanilla=vanilla)(augmented)\n    # Encode patches.\n    encoded_patches = PatchEncoder()(tokens)\n\n    # Create multiple layers of the Transformer block.\n    for _ in range(TRANSFORMER_LAYERS):\n        # Layer normalization 1.\n        x1 = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n        # Create a multi-head attention layer.\n        if not vanilla:\n            attention_output = MultiHeadAttentionLSA(\n                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n            )(x1, x1, attention_mask=diag_attn_mask)\n        else:\n            attention_output = layers.MultiHeadAttention(\n                num_heads=NUM_HEADS, key_dim=PROJECTION_DIM, dropout=0.1\n            )(x1, x1)\n        # Skip connection 1.\n        x2 = layers.Add()([attention_output, encoded_patches])\n        # Layer normalization 2.\n        x3 = layers.LayerNormalization(epsilon=1e-6)(x2)\n        # MLP.\n        x3 = mlp(x3, hidden_units=TRANSFORMER_UNITS, dropout_rate=0.1)\n        # Skip connection 2.\n        encoded_patches = layers.Add()([x3, x2])\n\n    # Create a [batch_size, projection_dim] tensor.\n    representation = layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n    representation = layers.Flatten()(representation)\n    representation = layers.Dropout(0.5)(representation)\n    # Add MLP.\n    features = mlp(representation, hidden_units=MLP_HEAD_UNITS, dropout_rate=0.5)\n    # Classify outputs.\n    logits = layers.Dense(NUM_CLASSES)(features)\n    # Create the Keras model.\n    model = keras.Model(inputs=inputs, outputs=logits)\n    return model","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:10:22.361730Z","iopub.execute_input":"2022-04-26T01:10:22.362205Z","iopub.status.idle":"2022-04-26T01:10:22.372584Z","shell.execute_reply.started":"2022-04-26T01:10:22.362163Z","shell.execute_reply":"2022-04-26T01:10:22.371251Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# Some code is taken from:\n# https://www.kaggle.com/ashusma/training-rfcx-tensorflow-tpu-effnet-b2.\nclass WarmUpCosine(keras.optimizers.schedules.LearningRateSchedule):\n    def __init__(\n        self, learning_rate_base, total_steps, warmup_learning_rate, warmup_steps\n    ):\n        super(WarmUpCosine, self).__init__()\n\n        self.learning_rate_base = learning_rate_base\n        self.total_steps = total_steps\n        self.warmup_learning_rate = warmup_learning_rate\n        self.warmup_steps = warmup_steps\n        self.pi = tf.constant(np.pi)\n\n    def __call__(self, step):\n        if self.total_steps < self.warmup_steps:\n            raise ValueError(\"Total_steps must be larger or equal to warmup_steps.\")\n\n        cos_annealed_lr = tf.cos(\n            self.pi\n            * (tf.cast(step, tf.float32) - self.warmup_steps)\n            / float(self.total_steps - self.warmup_steps)\n        )\n        learning_rate = 0.5 * self.learning_rate_base * (1 + cos_annealed_lr)\n\n        if self.warmup_steps > 0:\n            if self.learning_rate_base < self.warmup_learning_rate:\n                raise ValueError(\n                    \"Learning_rate_base must be larger or equal to \"\n                    \"warmup_learning_rate.\"\n                )\n            slope = (\n                self.learning_rate_base - self.warmup_learning_rate\n            ) / self.warmup_steps\n            warmup_rate = slope * tf.cast(step, tf.float32) + self.warmup_learning_rate\n            learning_rate = tf.where(\n                step < self.warmup_steps, warmup_rate, learning_rate\n            )\n        return tf.where(\n            step > self.total_steps, 0.0, learning_rate, name=\"learning_rate\"\n        )\n\n\ndef run_experiment(model):\n    total_steps = int((len(X_train) / BATCH_SIZE) * EPOCHS)\n    warmup_epoch_percentage = 0.10\n    warmup_steps = int(total_steps * warmup_epoch_percentage)\n    scheduled_lrs = WarmUpCosine(\n        learning_rate_base=LEARNING_RATE,\n        total_steps=total_steps,\n        warmup_learning_rate=0.0,\n        warmup_steps=warmup_steps,\n    )\n\n    optimizer = tfa.optimizers.AdamW(\n        learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n    )\n\n    model.compile(\n        optimizer=optimizer,\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=[\n            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n        ],\n    )\n\n    history = model.fit(\n        x=X_train,\n        y=Y_train,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS,\n        validation_split=0.1,\n    )\n    _, accuracy, top_5_accuracy = model.evaluate(X_test, Y_test, batch_size=BATCH_SIZE)\n    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n\n    return history\n\n\n# # Run experiments with the vanilla ViT\n# vit = create_vit_classifier(vanilla=True)\n# history = run_experiment(vit)\n\n# Run experiments with the Shifted Patch Tokenization and\n# Locality Self Attention modified ViT\nvit_sl = create_vit_classifier(vanilla=False)\nhistory = run_experiment(vit_sl)\n\nfrom sklearn.metrics import confusion_matrix\ntest_predictions= vit_sl.predict(X_test)\n\nconfusion= confusion_matrix(Y_test, np.argmax(test_predictions,axis=1))\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nax= plt.subplot()\nsns.heatmap(confusion, annot=True, fmt='g', ax=ax);\nax.set_title('Confusion Matrix of VT without Vanilla');\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-26T01:12:30.708856Z","iopub.execute_input":"2022-04-26T01:12:30.709171Z","iopub.status.idle":"2022-04-26T01:34:17.750750Z","shell.execute_reply.started":"2022-04-26T01:12:30.709137Z","shell.execute_reply":"2022-04-26T01:34:17.748934Z"},"trusted":true},"execution_count":19,"outputs":[]}]}